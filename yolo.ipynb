{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_test_input() missing 1 required positional argument: 'imgpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e181b12a7df3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../cfg/yolov3.cfg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../yolov3.weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_cfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../cfg/yolov3.cfg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_test_input() missing 1 required positional argument: 'imgpath'"
     ]
    }
   ],
   "source": [
    "model = Darknet(\"../cfg/yolov3.cfg\")\n",
    "model.load_weights(\"../yolov3.weights\")\n",
    "inp = get_test_input()\n",
    "blocks = parse_cfg(\"../cfg/yolov3.cfg\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175.42"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*8771/100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(inp, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "416/13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 85])\n",
      "torch.Size([1, 4, 85])\n",
      "torch.Size([85])\n",
      "torch.Size([85])\n",
      "torch.Size([85])\n",
      "torch.Size([85])\n"
     ]
    }
   ],
   "source": [
    "array=np.zeros([80])\n",
    "ar2=np.array([122,25,91,46,0.2])\n",
    "ar3=np.array([12,245,16,60,0.1])\n",
    "ar4=np.array([150,45,7,98,0.8])\n",
    "ar5=np.array([22,23,70,403,0.1])\n",
    "\n",
    "\n",
    "ar2=np.concatenate((ar2,array),axis=0)\n",
    "ar3=np.concatenate((ar3,array),axis=0)\n",
    "ar4=np.concatenate((ar4,array),axis=0)\n",
    "ar5=np.concatenate((ar5,array),axis=0)\n",
    "\n",
    "targets=torch.tensor([[ar2,ar3,ar4,ar5]])\n",
    "print(targets.shape)\n",
    "\n",
    "print(targets.shape)\n",
    "for obj in targets[0]:\n",
    "    print(obj.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors=3\n",
    "batch=0\n",
    "obj=0\n",
    "target=torch.stack([targets[batch,obj,0:4] for a in range(anchors)])\n",
    "target=target.type(torch.float)\n",
    "target=util.get_abs_coord(target)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymin</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, cell_type, xmin, xmax, ymin, ymax]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "\n",
    "import os, sys, random\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from shutil import copyfile\n",
    "\n",
    "annotations = sorted(glob('WOSDETC19/*.xml'))\n",
    "\n",
    "df = []\n",
    "cnt = 0\n",
    "for file in annotations:\n",
    "    prev_filename = file.split('/')[-1].split('.')[0] + '.jpg'\n",
    "    filename = str(cnt) + '.jpg'\n",
    "    row = []\n",
    "    parsedXML = ET.parse(file)\n",
    "    for node in parsedXML.getroot().iter('object'):\n",
    "        blood_cells = node.find('name').text\n",
    "        xmin = int(node.find('bndbox/xmin').text)\n",
    "        xmax = int(node.find('bndbox/xmax').text)\n",
    "        ymin = int(node.find('bndbox/ymin').text)\n",
    "        ymax = int(node.find('bndbox/ymax').text)\n",
    "        row = [prev_filename, filename, blood_cells, xmin, xmax,ymin, ymax]\n",
    "        df.append(row)\n",
    "    cnt += 1\n",
    "\n",
    "data = pd.DataFrame(df, columns=['prev_filename', 'filename', 'cell_type','xmin', 'xmax', 'ymin', 'ymax'])\n",
    "\n",
    "data[['filename', 'cell_type', 'xmin', 'xmax', 'ymin', 'ymax']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[0,2:]\n",
    "target[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_test_input() missing 1 required positional argument: 'imgpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4622565eeb03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_test_input() missing 1 required positional argument: 'imgpath'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "inp = get_test_input()\n",
    "\n",
    "pred = model(inp, torch.cuda.is_available())\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "loss=util.yolo_loss(pred,targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, random\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from shutil import copyfile\n",
    "\n",
    "annotations = sorted(glob('WOSDETC19/*.xml'))\n",
    "\n",
    "df = []\n",
    "cnt = 0\n",
    "for file in annotations:\n",
    "    row=[]\n",
    "    parsedXML = ET.parse(file)\n",
    "    root= parsedXML.getroot()\n",
    "    if (cnt!=1):\n",
    "        file=file.split('/')[-1].split('.')[0] + '.mp4'\n",
    "    else:\n",
    "        file=file.split('/')[-1].split('.')[0] + '.mpg'\n",
    "    for child in root[1][0]:\n",
    "        if(child.tag=='{http://lamp.cfar.umd.edu/viper#}object'):\n",
    "            for obj in child[0]:\n",
    "                filename=file.split('/')[-1].split('.')[0]\n",
    "                obj_id=child.attrib['id']\n",
    "                framespan=obj.attrib['framespan']\n",
    "                x=int(obj.attrib['x'])\n",
    "                y=int(obj.attrib['y'])\n",
    "                width=int(obj.attrib['width'])\n",
    "                height=int(obj.attrib['height'])\n",
    "                row = [filename, obj_id, framespan, x, y,width, height]\n",
    "                df.append(row)\n",
    "    cnt=cnt+1\n",
    "\n",
    "data = pd.DataFrame(df, columns=['filename', 'obj_id','framespan','x', 'y', 'width', 'height'])\n",
    "data.to_csv('annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# import pandas as pd\n",
    "# import os, sys, random\n",
    "# import xml.etree.ElementTree as ET\n",
    "# from glob import glob\n",
    "# import pandas as pd\n",
    "# from shutil import copyfile\n",
    "\n",
    "# annotations = sorted(glob('WOSDETC19/*.xml'))\n",
    "# cnt = 0\n",
    "# for file in annotations:\n",
    "#     row=[]\n",
    "#     parsedXML = ET.parse(file)\n",
    "#     root= parsedXML.getroot()\n",
    "#     file=file.split('/')[-1].split('.')[0] + '.mp4'\n",
    "#     cap = cv2.VideoCapture('WOSDETC19/'+file)\n",
    "    \n",
    "#     file_jpg=file.split('/')[-1].split('.')[0]\n",
    "    \n",
    "#     success = True\n",
    "#     while success:\n",
    "#         for child in root[1][0]:\n",
    "#             if(child.tag=='{http://lamp.cfar.umd.edu/viper#}object'):\n",
    "#                 for obj in child[0]:\n",
    "#                     framespan=obj.attrib['framespan']\n",
    "#                     count = int(framespan.split(':')[0])\n",
    "#                     cap.set(cv2.CAP_PROP_POS_FRAMES, count-1)\n",
    "#                     success, frame = cap.read()\n",
    "#                     cv2.imwrite(\"images/\"+file_jpg+\"_img%d.jpg\" % count, frame)     # save frame as JPEG file      \n",
    "#         success=False\n",
    "#     cnt=cnt+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024/32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>obj_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.179736</td>\n",
       "      <td>0.386403</td>\n",
       "      <td>-0.297138</td>\n",
       "      <td>-0.324744</td>\n",
       "      <td>-0.309528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obj_id</th>\n",
       "      <td>-0.179736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.065915</td>\n",
       "      <td>0.102089</td>\n",
       "      <td>-0.050285</td>\n",
       "      <td>-0.007669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.386403</td>\n",
       "      <td>-0.065915</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.375353</td>\n",
       "      <td>0.141238</td>\n",
       "      <td>0.141141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.297138</td>\n",
       "      <td>0.102089</td>\n",
       "      <td>-0.375353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.040109</td>\n",
       "      <td>-0.053514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>-0.324744</td>\n",
       "      <td>-0.050285</td>\n",
       "      <td>0.141238</td>\n",
       "      <td>-0.040109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>-0.309528</td>\n",
       "      <td>-0.007669</td>\n",
       "      <td>0.141141</td>\n",
       "      <td>-0.053514</td>\n",
       "      <td>0.978448</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0    obj_id         x         y     width    height\n",
       "Unnamed: 0    1.000000 -0.179736  0.386403 -0.297138 -0.324744 -0.309528\n",
       "obj_id       -0.179736  1.000000 -0.065915  0.102089 -0.050285 -0.007669\n",
       "x             0.386403 -0.065915  1.000000 -0.375353  0.141238  0.141141\n",
       "y            -0.297138  0.102089 -0.375353  1.000000 -0.040109 -0.053514\n",
       "width        -0.324744 -0.050285  0.141238 -0.040109  1.000000  0.978448\n",
       "height       -0.309528 -0.007669  0.141141 -0.053514  0.978448  1.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../annotations.csv')\n",
    "df.corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konsa15/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(518.8747, grad_fn=<AddBackward0>)\n",
      "tensor(334.6520, grad_fn=<AddBackward0>)\n",
      "tensor(296.8161, grad_fn=<AddBackward0>)\n",
      "tensor(288.6065, grad_fn=<AddBackward0>)\n",
      "tensor(289.0010, grad_fn=<AddBackward0>)\n",
      "tensor(290.6425, grad_fn=<AddBackward0>)\n",
      "tensor(292.0215, grad_fn=<AddBackward0>)\n",
      "tensor(292.9909, grad_fn=<AddBackward0>)\n",
      "tensor(293.6782, grad_fn=<AddBackward0>)\n",
      "tensor(294.1308, grad_fn=<AddBackward0>)\n",
      "tensor(294.4370, grad_fn=<AddBackward0>)\n",
      "tensor(294.6374, grad_fn=<AddBackward0>)\n",
      "tensor(294.7421, grad_fn=<AddBackward0>)\n",
      "tensor(294.7888, grad_fn=<AddBackward0>)\n",
      "tensor(294.7904, grad_fn=<AddBackward0>)\n",
      "tensor(294.7456, grad_fn=<AddBackward0>)\n",
      "tensor(294.6743, grad_fn=<AddBackward0>)\n",
      "tensor(294.5497, grad_fn=<AddBackward0>)\n",
      "tensor(294.3137, grad_fn=<AddBackward0>)\n",
      "tensor(293.8732, grad_fn=<AddBackward0>)\n",
      "tensor(293.1480, grad_fn=<AddBackward0>)\n",
      "tensor(291.8376, grad_fn=<AddBackward0>)\n",
      "tensor(289.2444, grad_fn=<AddBackward0>)\n",
      "tensor(283.1244, grad_fn=<AddBackward0>)\n",
      "tensor(275.8295, grad_fn=<AddBackward0>)\n",
      "tensor(283.5142, grad_fn=<AddBackward0>)\n",
      "tensor(149.7327, grad_fn=<AddBackward0>)\n",
      "tensor(329.7397, grad_fn=<AddBackward0>)\n",
      "tensor(247.6086, grad_fn=<AddBackward0>)\n",
      "tensor(252.1393, grad_fn=<AddBackward0>)\n",
      "tensor(252.3655, grad_fn=<AddBackward0>)\n",
      "tensor(249.2338, grad_fn=<AddBackward0>)\n",
      "tensor(226.1428, grad_fn=<AddBackward0>)\n",
      "tensor(275.6034, grad_fn=<AddBackward0>)\n",
      "tensor(280.9270, grad_fn=<AddBackward0>)\n",
      "tensor(291.2644, grad_fn=<AddBackward0>)\n",
      "tensor(284.9958, grad_fn=<AddBackward0>)\n",
      "tensor(290.3095, grad_fn=<AddBackward0>)\n",
      "tensor(292.4133, grad_fn=<AddBackward0>)\n",
      "tensor(293.3036, grad_fn=<AddBackward0>)\n",
      "tensor(293.6723, grad_fn=<AddBackward0>)\n",
      "tensor(293.8141, grad_fn=<AddBackward0>)\n",
      "tensor(293.7965, grad_fn=<AddBackward0>)\n",
      "tensor(293.6169, grad_fn=<AddBackward0>)\n",
      "tensor(293.2276, grad_fn=<AddBackward0>)\n",
      "tensor(292.5967, grad_fn=<AddBackward0>)\n",
      "tensor(291.6919, grad_fn=<AddBackward0>)\n",
      "tensor(290.2274, grad_fn=<AddBackward0>)\n",
      "tensor(287.7288, grad_fn=<AddBackward0>)\n",
      "tensor(283.6049, grad_fn=<AddBackward0>)\n",
      "tensor(278.0563, grad_fn=<AddBackward0>)\n",
      "tensor(280.6510, grad_fn=<AddBackward0>)\n",
      "tensor(287.2223, grad_fn=<AddBackward0>)\n",
      "tensor(276.5089, grad_fn=<AddBackward0>)\n",
      "tensor(278.5320, grad_fn=<AddBackward0>)\n",
      "tensor(281.7282, grad_fn=<AddBackward0>)\n",
      "tensor(283.5850, grad_fn=<AddBackward0>)\n",
      "tensor(284.3137, grad_fn=<AddBackward0>)\n",
      "tensor(284.3471, grad_fn=<AddBackward0>)\n",
      "tensor(283.9852, grad_fn=<AddBackward0>)\n",
      "tensor(283.4743, grad_fn=<AddBackward0>)\n",
      "tensor(283.0549, grad_fn=<AddBackward0>)\n",
      "tensor(282.9599, grad_fn=<AddBackward0>)\n",
      "tensor(282.8905, grad_fn=<AddBackward0>)\n",
      "tensor(281.9873, grad_fn=<AddBackward0>)\n",
      "tensor(280.0950, grad_fn=<AddBackward0>)\n",
      "tensor(277.9273, grad_fn=<AddBackward0>)\n",
      "tensor(276.4041, grad_fn=<AddBackward0>)\n",
      "tensor(275.8270, grad_fn=<AddBackward0>)\n",
      "tensor(276.0414, grad_fn=<AddBackward0>)\n",
      "tensor(276.6113, grad_fn=<AddBackward0>)\n",
      "tensor(277.1285, grad_fn=<AddBackward0>)\n",
      "tensor(277.5946, grad_fn=<AddBackward0>)\n",
      "tensor(278.1458, grad_fn=<AddBackward0>)\n",
      "tensor(278.6989, grad_fn=<AddBackward0>)\n",
      "tensor(279.1109, grad_fn=<AddBackward0>)\n",
      "tensor(279.2447, grad_fn=<AddBackward0>)\n",
      "tensor(279.1997, grad_fn=<AddBackward0>)\n",
      "tensor(279.2378, grad_fn=<AddBackward0>)\n",
      "tensor(279.3773, grad_fn=<AddBackward0>)\n",
      "tensor(279.2800, grad_fn=<AddBackward0>)\n",
      "tensor(278.9734, grad_fn=<AddBackward0>)\n",
      "tensor(278.7079, grad_fn=<AddBackward0>)\n",
      "tensor(67.1986, grad_fn=<AddBackward0>)\n",
      "tensor(238.3528, grad_fn=<AddBackward0>)\n",
      "tensor(222.5354, grad_fn=<AddBackward0>)\n",
      "tensor(13.8361, grad_fn=<AddBackward0>)\n",
      "tensor(124.3917, grad_fn=<AddBackward0>)\n",
      "tensor(38.3191, grad_fn=<AddBackward0>)\n",
      "tensor(35.7672, grad_fn=<AddBackward0>)\n",
      "tensor(321.5696, grad_fn=<AddBackward0>)\n",
      "tensor(282.1562, grad_fn=<AddBackward0>)\n",
      "tensor(291.9547, grad_fn=<AddBackward0>)\n",
      "tensor(295.1422, grad_fn=<AddBackward0>)\n",
      "tensor(295.9092, grad_fn=<AddBackward0>)\n",
      "tensor(296.1133, grad_fn=<AddBackward0>)\n",
      "tensor(296.1783, grad_fn=<AddBackward0>)\n",
      "tensor(296.2067, grad_fn=<AddBackward0>)\n",
      "tensor(296.2185, grad_fn=<AddBackward0>)\n",
      "tensor(296.2229, grad_fn=<AddBackward0>)\n",
      "tensor(296.2249, grad_fn=<AddBackward0>)\n",
      "tensor(296.2244, grad_fn=<AddBackward0>)\n",
      "tensor(296.2209, grad_fn=<AddBackward0>)\n",
      "tensor(296.2140, grad_fn=<AddBackward0>)\n",
      "tensor(296.2030, grad_fn=<AddBackward0>)\n",
      "tensor(296.1895, grad_fn=<AddBackward0>)\n",
      "tensor(296.1746, grad_fn=<AddBackward0>)\n",
      "tensor(296.1578, grad_fn=<AddBackward0>)\n",
      "tensor(296.1392, grad_fn=<AddBackward0>)\n",
      "tensor(296.1147, grad_fn=<AddBackward0>)\n",
      "tensor(296.0828, grad_fn=<AddBackward0>)\n",
      "tensor(296.0436, grad_fn=<AddBackward0>)\n",
      "tensor(295.9961, grad_fn=<AddBackward0>)\n",
      "tensor(295.9387, grad_fn=<AddBackward0>)\n",
      "tensor(295.8748, grad_fn=<AddBackward0>)\n",
      "tensor(295.7996, grad_fn=<AddBackward0>)\n",
      "tensor(295.7159, grad_fn=<AddBackward0>)\n",
      "tensor(295.6308, grad_fn=<AddBackward0>)\n",
      "tensor(295.5485, grad_fn=<AddBackward0>)\n",
      "tensor(295.4778, grad_fn=<AddBackward0>)\n",
      "tensor(295.4245, grad_fn=<AddBackward0>)\n",
      "tensor(295.3893, grad_fn=<AddBackward0>)\n",
      "tensor(295.3696, grad_fn=<AddBackward0>)\n",
      "tensor(295.3579, grad_fn=<AddBackward0>)\n",
      "tensor(295.3505, grad_fn=<AddBackward0>)\n",
      "tensor(295.3459, grad_fn=<AddBackward0>)\n",
      "tensor(295.3429, grad_fn=<AddBackward0>)\n",
      "tensor(295.3411, grad_fn=<AddBackward0>)\n",
      "tensor(295.3399, grad_fn=<AddBackward0>)\n",
      "tensor(295.3392, grad_fn=<AddBackward0>)\n",
      "tensor(295.3388, grad_fn=<AddBackward0>)\n",
      "tensor(295.3385, grad_fn=<AddBackward0>)\n",
      "tensor(295.3384, grad_fn=<AddBackward0>)\n",
      "tensor(295.3383, grad_fn=<AddBackward0>)\n",
      "tensor(295.3382, grad_fn=<AddBackward0>)\n",
      "tensor(295.3382, grad_fn=<AddBackward0>)\n",
      "tensor(295.3381, grad_fn=<AddBackward0>)\n",
      "tensor(295.3381, grad_fn=<AddBackward0>)\n",
      "tensor(295.3381, grad_fn=<AddBackward0>)\n",
      "tensor(295.3381, grad_fn=<AddBackward0>)\n",
      "tensor(295.3381, grad_fn=<AddBackward0>)\n",
      "tensor(295.3381, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3380, grad_fn=<AddBackward0>)\n",
      "tensor(295.3379, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3e079b5ef396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1980\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1080\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1980\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'height'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1080\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolo_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/notebooks/drone_vs_bird/yolo_drone_vs_bird/darknet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, CUDA)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"convolutional\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodule_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"upsample\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodule_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"route\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util\n",
    "import torch.optim as optim\n",
    "\n",
    "model = Darknet(\"../cfg/yolov3.cfg\")\n",
    "model.load_weights(\"../yolov3.weights\")\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00001)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "batch=1\n",
    "k=0\n",
    "cost=0\n",
    "for index, row in df.iterrows():\n",
    "#     imgpath='../images/'+row['filename']+'_img'+row['framespan'].split(':')[0]+'.jpg'\n",
    "#     inp = get_test_input(imgpath)\n",
    "#     targets=torch.tensor([[[row['x']*(416/1980),row['y']*(416/1080),row['width']*(416/1980),row['height']*(416/1080),1,1]]])\n",
    "#     pred = model(inp, torch.cuda.is_available())\n",
    "#     loss=util.yolo_loss(pred,targets)\n",
    "#     print(loss)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "    index=1\n",
    "    imgpath='../images/'+df['filename'][index]+'_img'+df['framespan'][index].split(':')[0]+'.jpg'\n",
    "    inp = get_test_input(imgpath)\n",
    "    targets=torch.tensor([[[df['x'][index]*(416/1980),df['y'][index]*(416/1080),df['width'][index]*(416/1980),df['height'][index]*(416/1080),1,1]]])\n",
    "    pred = model(inp, torch.cuda.is_available())\n",
    "    loss=util.yolo_loss(pred,targets)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#     cost=cost+loss\n",
    "#     if(k==batch):\n",
    "#         print(cost)\n",
    "#         cost.backward()\n",
    "#         optimizer.step()\n",
    "#         cost=0\n",
    "#         k=0\n",
    "#     k=k+1\n",
    "    \n",
    "\n",
    "   \n",
    "index=10\n",
    "imgpath='../images/'+df['filename'][index]+'_img'+df['framespan'][index].split(':')[0]+'.jpg'\n",
    "inp = get_test_input(imgpath)\n",
    "targets=torch.tensor([[[df['x'][index]*(416/1980),df['y'][index]*(416/1080),df['width'][index]*(416/1980),df['height'][index]*(416/1080),1,1]]])\n",
    "pred = model(inp, torch.cuda.is_available())\n",
    "print(pred)\n",
    "loss=util.yolo_loss(pred,targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=torch.tensor([123.5394, 284.6519,   5.2525,   6.9333,   1.0000,   1.0000])\n",
    "box=torch.tensor([[9.6000e+01, 2.6563e+02, 4.5147e+01, 5.6191e+01, 2.7485e-04, 1.0556e-02],\n",
    "        [9.6394e+01, 2.5649e+02, 1.3267e+00, 3.3532e-01, 2.0614e-03, 8.3126e-03],\n",
    "        [9.6071e+01, 2.5609e+02, 1.1885e+00, 4.8042e-01, 2.5095e-03, 2.6043e-03]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 72.4265, 236.5345, 117.5735, 292.7255],\n",
      "        [ 94.7306, 255.3223,  96.0574, 255.6577],\n",
      "        [ 94.4767, 254.8498,  95.6653, 255.3302]])\n"
     ]
    }
   ],
   "source": [
    "absolute_box=util.get_abs_coord(box[:,0:4])\n",
    "mask=get_mask(obj[0:2],13,416,3)\n",
    "\n",
    "print(absolute_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "target_box=torch.stack([obj[0:4] for a in range(3)])\n",
    "target_box=target_box.type(torch.float)\n",
    "target_box=get_abs_coord(target_box)\n",
    "\n",
    "iou=util.bbox_iou(target_box,absolute_box)\n",
    "\n",
    "iou_mask=iou.max() == iou\n",
    "box=box[iou_mask,:]\n",
    "iou_value=iou.max()\n",
    "print(iou_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.6000e+01, 2.6563e+02, 4.5147e+01, 5.6191e+01, 2.7485e-04, 1.0556e-02])\n"
     ]
    }
   ],
   "source": [
    "print(box[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(0.0240, grad_fn=<AddBackward0>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "\n",
    "for x in pbar(range(10)):\n",
    "    x==x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress is 9"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-56d7624e8dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\rProgress is %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "for i in range(100):\n",
    "    sys.stdout.write('\\rProgress is %d' %i)\n",
    "    sys.stdout.flush()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konsa15/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress is 0.0% loss is: 281.5664367675781 Time remaining is 0.0 min"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ab0bb603537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolo_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\rProgress is '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprg_counter\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m8771\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'%'\u001b[0m \u001b[0;34m' loss is: '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' Time remaining is '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimated\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../annotations.csv')\n",
    "\n",
    "model = Darknet(\"../cfg/yolov3.cfg\")\n",
    "model.load_weights(\"../yolov3.weights\")\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "batch=1\n",
    "\n",
    "epochs=20\n",
    "start = timeit.default_timer()\n",
    "\n",
    "lock=0\n",
    "remaining=0\n",
    "estimated=0\n",
    "for e in range(epochs):\n",
    "    prg_counter=0\n",
    "    print(\"epoch \"+str(e))\n",
    "    for index, row in df.iterrows():\n",
    "        imgpath='../images/'+row['filename']+'_img'+row['framespan'].split(':')[0]+'.jpg'\n",
    "        inp = get_test_input(imgpath)\n",
    "        targets=torch.tensor([[[row['x']*(416/1980),row['y']*(416/1080),row['width']*(416/1980),row['height']*(416/1080),1,1]]])\n",
    "        pred = model(inp, torch.cuda.is_available())\n",
    "        loss=util.yolo_loss(pred,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sys.stdout.write('\\rProgress is ' +str(prg_counter/8771*100)+'%' ' loss is: '+ str(loss.item())+' Time remaining is ' +str(estimated/60)+' min')\n",
    "        sys.stdout.flush()\n",
    "        prg_counter=prg_counter+1\n",
    "        if ((prg_counter/8771*100>1)&(lock==0)):\n",
    "            stop = timeit.default_timer()\n",
    "            estimated= (stop-start)*99\n",
    "            dur=stop-start\n",
    "            start=stop\n",
    "            lock=1\n",
    "        if estimated!=0:\n",
    "            stop = timeit.default_timer()\n",
    "            dur=stop-start\n",
    "            start=stop\n",
    "            estimated=estimated-dur\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.437082052230835"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=time.time()\n",
    "for x in range(100000000):\n",
    "    x=x+x\n",
    "time.time()-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True, False])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "iou=torch.tensor([1,4,2])\n",
    "iou_mask=iou.max() == iou\n",
    "print(iou_mask)\n",
    "box1=torch.tensor([[1,2,3],[2,2,3],[5,3,5]])\n",
    "box1=box1[iou_mask,:]\n",
    "iou_value=iou.max()\n",
    "print(box1.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
